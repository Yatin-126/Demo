{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DPzErlXBS8uL",
        "outputId": "f1747041-bd46-4396-f296-a99f09d7edc8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset loaded successfully.\n",
            "\n",
            "Training models...\n",
            "Models trained.\n",
            "\n",
            "--- Model Performance ---\n",
            "Model: Linear Regression\n",
            "  R-squared ($R^2$): 0.6688\n",
            "  Root Mean Squared Error (RMSE): 4.9286\n",
            "\n",
            "Model: Ridge Regression\n",
            "  R-squared ($R^2$): 0.6685\n",
            "  Root Mean Squared Error (RMSE): 4.9308\n",
            "\n",
            "Model: Lasso Regression\n",
            "  R-squared ($R^2$): 0.6501\n",
            "  Root Mean Squared Error (RMSE): 5.0652\n",
            "\n",
            "\n",
            "--- Coefficient Analysis ---\n",
            "\n",
            "Could not generate coefficient plot: This OneHotEncoder instance is not fitted yet. Call 'fit' with appropriate arguments before using this estimator.\n",
            "This step requires the model to be trained successfully on data with categorical features.\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Model imports\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LinearRegression, Ridge, Lasso\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "\n",
        "# Preprocessing imports\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.impute import SimpleImputer\n",
        "\n",
        "# Set plot style\n",
        "sns.set(style=\"whitegrid\")\n",
        "\n",
        "# -------------------------------------------------\n",
        "# 1. Load Data\n",
        "# -------------------------------------------------\n",
        "# We use a direct URL to the 'train.csv' file from the Kaggle competition\n",
        "data_url = \"https_url_to_ames_housing_train.csv\" # Placeholder: In a real environment, you'd replace this with the actual URL or local path.\n",
        "# For a runnable example, let's use a common public version of this dataset:\n",
        "data_url = \"https://raw.githubusercontent.com/jbrownlee/Datasets/master/housing.csv\"\n",
        "# NOTE: The above URL is for the *Boston* housing dataset, which is smaller but will run instantly.\n",
        "# The logic is identical. Let's proceed with the Boston data for a quick, runnable example.\n",
        "# If you use the Ames dataset, the principles are the same but the feature names will differ.\n",
        "column_names = ['CRIM', 'ZN', 'INDUS', 'CHAS', 'NOX', 'RM', 'AGE', 'DIS', 'RAD', 'TAX', 'PTRATIO', 'B', 'LSTAT', 'MEDV']\n",
        "try:\n",
        "    data = pd.read_csv(data_url, header=None, sep=',', names=column_names)\n",
        "    print(\"Dataset loaded successfully.\")\n",
        "\n",
        "    # Define features (X) and target (y)\n",
        "    # MEDV (Median value of owner-occupied homes) is our target\n",
        "    X = data.drop('MEDV', axis=1)\n",
        "    y = data['MEDV']\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"Error loading data: {e}\")\n",
        "    print(\"Please download the 'train.csv' from the Ames housing link and load it locally.\")\n",
        "    # As a fallback, create dummy data to show the code structure\n",
        "    # X = pd.DataFrame(np.random.rand(100, 20), columns=[f'feat_{i}' for i in range(20)])\n",
        "    # X['cat_feat_1'] = np.random.choice(['A', 'B', 'C'], 100)\n",
        "    # X['cat_feat_2'] = np.random.choice(['X', 'Y'], 100)\n",
        "    # y = pd.Series(np.random.rand(100) * 1000)\n",
        "\n",
        "# --- RE-ADAPTATION FOR AMES DATASET (if you load it manually) ---\n",
        "# If you load the Ames 'train.csv' locally:\n",
        "# data = pd.read_csv('train.csv')\n",
        "# X = data.drop(['Id', 'SalePrice'], axis=1)\n",
        "# # Log-transform the target variable for better performance (it's skewed)\n",
        "# y = np.log1p(data['SalePrice'])\n",
        "# -----------------------------------------------------------------\n",
        "\n",
        "\n",
        "# -------------------------------------------------\n",
        "# 2. Preprocessing Pipeline\n",
        "# -------------------------------------------------\n",
        "# We must scale numerical features and one-hot encode categorical features.\n",
        "# Regularization is sensitive to the scale of features.\n",
        "\n",
        "# Identify numerical and categorical features\n",
        "# (This is a simplified list for the Boston dataset)\n",
        "numerical_features = X.select_dtypes(include=np.number).columns\n",
        "categorical_features = X.select_dtypes(include='object').columns\n",
        "\n",
        "# Create a preprocessing pipeline for numerical features\n",
        "numeric_transformer = Pipeline(steps=[\n",
        "    ('imputer', SimpleImputer(strategy='median')),  # Fill missing values\n",
        "    ('scaler', StandardScaler())                   # Scale data\n",
        "])\n",
        "\n",
        "# Create a preprocessing pipeline for categorical features\n",
        "categorical_transformer = Pipeline(steps=[\n",
        "    ('imputer', SimpleImputer(strategy='most_frequent')), # Fill missing values\n",
        "    ('onehot', OneHotEncoder(handle_unknown='ignore'))     # One-hot encode\n",
        "])\n",
        "\n",
        "# Combine pipelines using ColumnTransformer\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('num', numeric_transformer, numerical_features),\n",
        "        ('cat', categorical_transformer, categorical_features)\n",
        "    ])\n",
        "\n",
        "# -------------------------------------------------\n",
        "# 3. Create and Train Models\n",
        "# -------------------------------------------------\n",
        "\n",
        "# Split data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create full pipelines for each model\n",
        "# The pipeline will first preprocess the data, then apply the model\n",
        "\n",
        "# Model 1: Standard Linear Regression (Baseline)\n",
        "lr_pipeline = Pipeline(steps=[('preprocessor', preprocessor),\n",
        "                              ('model', LinearRegression())])\n",
        "\n",
        "# Model 2: Ridge Regression (L2 Regularization)\n",
        "# alpha is the regularization strength.\n",
        "ridge_pipeline = Pipeline(steps=[('preprocessor', preprocessor),\n",
        "                                 ('model', Ridge(alpha=1.0))])\n",
        "\n",
        "# Model 3: Lasso Regression (L1 Regularization)\n",
        "# A higher alpha is often needed for Lasso to show significant feature selection\n",
        "lasso_pipeline = Pipeline(steps=[('preprocessor', preprocessor),\n",
        "                                 ('model', Lasso(alpha=0.1))]) # Start with 0.1 for this dataset\n",
        "\n",
        "# Train models\n",
        "print(\"\\nTraining models...\")\n",
        "lr_pipeline.fit(X_train, y_train)\n",
        "ridge_pipeline.fit(X_train, y_train)\n",
        "lasso_pipeline.fit(X_train, y_train)\n",
        "print(\"Models trained.\")\n",
        "\n",
        "# -------------------------------------------------\n",
        "# 4. Evaluate Performance\n",
        "# -------------------------------------------------\n",
        "models = {\n",
        "    \"Linear Regression\": lr_pipeline,\n",
        "    \"Ridge Regression\": ridge_pipeline,\n",
        "    \"Lasso Regression\": lasso_pipeline\n",
        "}\n",
        "\n",
        "print(\"\\n--- Model Performance ---\")\n",
        "results = {}\n",
        "\n",
        "for name, model in models.items():\n",
        "    # Make predictions\n",
        "    y_pred = model.predict(X_test)\n",
        "\n",
        "    # --- If using Ames data with log-transform ---\n",
        "    # y_pred = np.expm1(y_pred)\n",
        "    # y_test_orig = np.expm1(y_test)\n",
        "    # rmse = np.sqrt(mean_squared_error(y_test_orig, y_pred))\n",
        "    # r2 = r2_score(y_test_orig, y_pred)\n",
        "    # ---------------------------------------------\n",
        "\n",
        "    # --- For Boston data (no log-transform) ---\n",
        "    rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
        "    r2 = r2_score(y_test, y_pred)\n",
        "    # ---------------------------------------------\n",
        "\n",
        "    print(f\"Model: {name}\")\n",
        "    print(f\"  R-squared ($R^2$): {r2:.4f}\")\n",
        "    print(f\"  Root Mean Squared Error (RMSE): {rmse:.4f}\\n\")\n",
        "\n",
        "    results[name] = {'r2': r2, 'rmse': rmse, 'model_obj': model}\n",
        "\n",
        "\n",
        "# -------------------------------------------------\n",
        "# 5. Analyze Regularization Effect (Coefficients)\n",
        "# -------------------------------------------------\n",
        "print(\"\\n--- Coefficient Analysis ---\")\n",
        "\n",
        "# Get feature names after preprocessing\n",
        "# This is tricky but essential for interpretation\n",
        "try:\n",
        "    # Ensure preprocessor is fitted for direct access to its components\n",
        "    preprocessor.fit(X_train)\n",
        "\n",
        "    # Initialize list for categorical feature names\n",
        "    cat_features_out = []\n",
        "    # Only try to get names from OneHotEncoder if there are actual categorical features\n",
        "    if len(categorical_features) > 0:\n",
        "        cat_features_out = preprocessor.named_transformers_['cat'] \\\n",
        "                                      .named_steps['onehot'] \\\n",
        "                                      .get_feature_names_out(categorical_features)\n",
        "\n",
        "    # Combine with numerical feature names\n",
        "    feature_names = list(numerical_features) + list(cat_features_out)\n",
        "\n",
        "    # Get coefficients\n",
        "    lr_coefs = results[\"Linear Regression\"]['model_obj'].named_steps['model'].coef_\n",
        "    ridge_coefs = results[\"Ridge Regression\"]['model_obj'].named_steps['model'].coef_\n",
        "    lasso_coefs = results[\"Lasso Regression\"]['model_obj'].named_steps['model'].coef_\n",
        "\n",
        "    # Create a DataFrame for comparison\n",
        "    coef_df = pd.DataFrame({\n",
        "        'Feature': feature_names,\n",
        "        'LinearReg': lr_coefs,\n",
        "        'Ridge': ridge_coefs,\n",
        "        'Lasso': lasso_coefs\n",
        "    })\n",
        "\n",
        "    # Analyze the number of zero coefficients\n",
        "    print(f\"Total features after one-hot encoding: {len(feature_names)}\")\n",
        "    print(f\"Linear Regression non-zero coefficients: {np.sum(lr_coefs != 0)}\")\n",
        "    print(f\"Ridge Regression non-zero coefficients: {np.sum(ridge_coefs != 0)}\")\n",
        "    print(f\"Lasso Regression non-zero coefficients: {np.sum(lasso_coefs != 0)}\")\n",
        "    print(f\"Lasso eliminated {np.sum(lasso_coefs == 0)} features.\")\n",
        "\n",
        "    # Plot coefficient magnitudes\n",
        "    plt.figure(figsize=(15, 6))\n",
        "\n",
        "    # Plot Lasso coefficients\n",
        "    plt.subplot(1, 2, 1)\n",
        "    lasso_coefs_sorted = pd.Series(lasso_coefs, index=feature_names).sort_values()\n",
        "    lasso_coefs_sorted[lasso_coefs_sorted != 0].plot(kind='barh', title='Lasso Coefficients (L1)')\n",
        "\n",
        "    # Plot Ridge coefficients\n",
        "    plt.subplot(1, 2, 2)\n",
        "    ridge_coefs_sorted = pd.Series(ridge_coefs, index=feature_names).sort_values()\n",
        "    ridge_coefs_sorted.plot(kind='barh', title='Ridge Coefficients (L2)', color='skyblue')\n",
        "\n",
        "    plt.suptitle(\"Effect of Regularization on Model Coefficients\")\n",
        "    plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
        "    plt.show()\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"\\nCould not generate coefficient plot: {e}\")\n",
        "    print(\"This step requires the model to be trained successfully on data with categorical features.\")"
      ]
    }
  ]
}